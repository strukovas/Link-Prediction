# -*- coding: utf-8 -*-
"""LinkPrediction_Strukova2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1suhMJrJrTUMEYaQuHj0d5qnnCXawChsF
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -U imbalanced-learn

import csv
import numpy as np
import matplotlib.pyplot as plt
import scipy as sp
import networkx as nx
import pandas as pd
import random

import os
import urllib.request
import io
import gzip

from IPython.display import Image
from io import StringIO 
from imblearn.under_sampling import RandomUnderSampler
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from collections import Counter


# %matplotlib inline

currentDirectory = os.getcwd()
print("Current dir: "+currentDirectory)

FILE_URL = "https://datasets.imdbws.com/name.basics.tsv.gz"
response = urllib.request.urlopen(FILE_URL)
comp_names = io.BytesIO(response.read())

FILE_URL = "https://datasets.imdbws.com/title.basics.tsv.gz"
response = urllib.request.urlopen(FILE_URL)
comp_titles = io.BytesIO(response.read())

decomp_names = gzip.GzipFile(fileobj=comp_names).read()[:30_000_000].decode('utf-8')
decomp_titles = gzip.GzipFile(fileobj=comp_titles).read()[:30_000_000].decode('utf-8')

nNames = decomp_names.count("\n")
nTitles = decomp_titles.count("\n")

sNames = min(nNames,1_000_000)
sTitles = min(nTitles,1_000_000)
skipNames = []#sorted(random.sample(range(1,nNames+1),nNames-sNames))
skipTitles = []#sorted(random.sample(range(1,nTitles+1),nTitles-sTitles))

namesS = StringIO(decomp_names)
titlesS = StringIO(decomp_titles)

titles = pd.read_csv(titlesS, delimiter='\t', usecols=["tconst","startYear"], skiprows=skipTitles)[:-1]

yearOfFilm = {}
actorsInFilm = {}

#Associate each film with each year
print("Processing films:")
for i, row in titles.iterrows():
    if i % 20_000 == 0: print("Film "+str(i)+"/"+str(len(titles)))
    try:
        yearOfFilm[row['tconst']] = int(row['startYear'])
        actorsInFilm[row['tconst']] = set()
    except:
        pass

del titles

names = pd.read_csv(namesS, delimiter='\t', usecols=["nconst","knownForTitles"], skiprows=skipNames)[:-1]

#For each actor
print("Processing actors:")
for i, row in names.iterrows():
    if i % 20_000 == 0: print("Actor "+str(i)+"/"+str(len(names)))
    actor = row['nconst']

    # Go to each film were he appears and put him in the list of actors that appear in that film
    for film in row['knownForTitles'].split(','):
        #if film not in actorsInFilm: #If in the dataset of films we do not have that film
        #    actorsInFilm[film] = set()
        #    yearOfFilm[film] = 1900 #Put 1900 as dummy year       
        if film in actorsInFilm:     
            actorsInFilm[film].add(actor)

del names

minYear = 1890
maxYear = 2025

def nKIntervals(k):
    return np.ceil((maxYear-minYear)/k).astype(int)

#Function to know given a year to what graph corresponds
#Example: If k=5 and year is 1890 then index is 0, but if year is 1912 then index is 5
def getGraphId(year, k):
    if (year < minYear):
        return 0
    if (year > maxYear):
        return nKIntervals(k)-1
    return (year - minYear) // k

def getYearFromId(id, k):
    return minYear + id*k

#Given a value K, it return a list of graphs
#Were graphs[0] are films from minYear to minYear+k, graphs[1] from minYear+k to minYear+2k ...
def getGraphsKYears(k):
    #Min and max year that we are considering
    graphs = [nx.Graph() for x in range(nKIntervals(k))] #Create list of ggraphs

    #For each film
    print("Processing films")
    for i, (film, year) in enumerate(yearOfFilm.items()):
        if i % 80_000 == 0: print("Film "+str(i)+"/"+str(len(yearOfFilm)))
        graphId = getGraphId(year,k)

        #For every pair of actors we put an edge between them
        for actor1 in actorsInFilm[film]:
            graphs[graphId].add_node(actor1)
            for actor2 in actorsInFilm[film]:
                if actor1 <= actor2: continue
                graphs[graphId].add_edge(actor1, actor2)#, label=film)
    return graphs

K = 5
graphsKYears = getGraphsKYears(K)

from collections import defaultdict

v = defaultdict(list)

z = [0]*((maxYear-minYear)//K)

for film, year in sorted(yearOfFilm.items()):
    v[getGraphId(year, K)].append(film)
    z[getGraphId(year, K)] = len(v[getGraphId(year, K)])

plt.bar(range(1890,1890+len(z)*K,K),z, width=2)
plt.ylabel("Frequence")
plt.xlabel("Degree")
plt.title("Films per K")

edgesPerKYears = [len(g.edges()) for g in graphsKYears]
plt.bar(range(minYear,maxYear,K),edgesPerKYears, width=2)
plt.ylabel("Frequence")
plt.xlabel("Degree")
plt.title("Edges per K")

actorsPerKYears = [len(g.nodes()) for g in graphsKYears]
plt.bar(range(minYear,maxYear,K),actorsPerKYears, width=2)
plt.ylabel("Frequence")
plt.xlabel("Degree")
plt.title("Actors per K")

actorPairs = []
isEdge = []

for gId in range(len(graphsKYears)-1):
    currG = nx.Graph(graphsKYears[gId])
    nextG = nx.Graph(graphsKYears[gId+1])
    print("----------------------------")
    print("Period "+str((1890+K*gId)))
    if len(nextG.edges()) < 100 or len(currG.edges()) < 100:
        print("Skip")
        continue

    #nMaxActors = 100_000
    #currG.remove_nodes_from(list(currG.nodes())[nMaxActors:])

    periodPairs = []

    print("Processing Edges")
    maxSizeDataset = 7_000
    for i, (actor1,actor2) in enumerate(sorted(nextG.edges(), key=lambda _: random.random())):
        if len(periodPairs) >= maxSizeDataset : break
        if not (currG.has_node(actor1) and currG.has_node(actor2)):
            continue

        periodPairs.append([actor1,actor2,gId])
        isEdge.append(True)

    maxSizeDataset = 2*len(periodPairs)
    nActors = len(currG.nodes())
    nPairsPerActor = (maxSizeDataset // nActors)+1

    print("Processing Actors")
    for i, actor1 in enumerate(sorted(currG.nodes(), key=lambda _: random.random())):
        if len(periodPairs) > maxSizeDataset: break
        j = 0
        for actor2 in random.sample(currG.nodes(),min(len(currG.nodes),nPairsPerActor*2)):
            thereIsEdge = nextG.has_node(actor1) and nextG.has_node(actor2) and nextG.has_edge(actor1,actor2)
            if actor1 <= actor2 or thereIsEdge: continue
            if j > nPairsPerActor: break

            periodPairs.append([actor1,actor2,gId])
            isEdge.append(False)
            j +=1
            if len(periodPairs) % 5_000 == 0: print("Actor "+str(i)+"/"+str(nActors))

    actorPairs += periodPairs

print()
print('Dataset pairs with edge %s' % Counter(isEdge))
rus = RandomUnderSampler(random_state=42)
actorPairs, isEdge = rus.fit_resample(actorPairs, isEdge)
print('Resampled dataset pairs with edge %s' % Counter(isEdge))
#print()

actorPairsByPeriod = {}
for (actor1, actor2, gId), edge in zip(actorPairs, isEdge):
    actorPairsByPeriod.setdefault(int(gId), []).append([actor1,actor2,edge])

for key in actorPairsByPeriod.keys():
    actorPairsByPeriod[key] = np.array(actorPairsByPeriod[key])
    np.random.shuffle(actorPairsByPeriod[key])

pairMetricsByPeriod = {}
for gId, actorPairs in sorted(actorPairsByPeriod.items()):
    pairMetrics = []
    graph = graphsKYears[int(gId)]
    for i, (actor1, actor2, edge) in enumerate(actorPairs):
        jaccard = list(nx.jaccard_coefficient(graph, ebunch=[[actor1,actor2]]))[0][2]
        adamic = list(nx.adamic_adar_index(graph, ebunch=[[actor1,actor2]]))[0][2]
        pref_attach = list(nx.preferential_attachment(graph, ebunch=[[actor1,actor2]]))[0][2]
        resource_alloc = list(nx.resource_allocation_index(graph, ebunch=[[actor1,actor2]]))[0][2]
        try:
            shortest_path = nx.shortest_path_length(graph, source=actor1, target=actor2)
        except:
            shortest_path = 10_000 #No path

        pairMetrics.append([jaccard,adamic,pref_attach,resource_alloc,shortest_path,edge])
        
        if i % 7000 == 0:
            print("Period "+str(gId)+", Iteration "+str(i)+"/"+str(len(actorPairs)))

    pairMetricsByPeriod[gId] = np.array(pairMetrics)
    np.random.shuffle(pairMetricsByPeriod[gId])

periods = sorted(pairMetricsByPeriod.keys())

for i in range(len(periods)-1):
    period = periods[i]
    nextPeriod =  periods[i+1]
    if period+1 != nextPeriod:
        print("Period "+str(period)+" exists but not period "+str(period+1))
        continue
    if len(pairMetricsByPeriod[period]) < 100 or len(pairMetricsByPeriod[nextPeriod]) < 100:
        print("Period "+str(period)+" to "+str(period+1) + " doesnt have enough samples")
        continue
    x_train = pairMetricsByPeriod[period][:,:-1]
    y_train = pairMetricsByPeriod[period][:,-1]
    x_test = pairMetricsByPeriod[nextPeriod][:,:-1]
    y_test = pairMetricsByPeriod[nextPeriod][:,-1]

    print("Training SVM. Year "+str(getYearFromId(period,K))+" to "+str(getYearFromId(nextPeriod,K)))
    clf = SVC(gamma='auto')
    clf.fit(x_train, y_train)

    y_pred = clf.predict(x_test)

    pred = Counter(y_test == y_pred)

    print("Predictions: "+str(100 * pred[True]/(pred[False] + pred[True]))+"%")
    print(str(pred))
    print()

"""For each year predict the same year"""

periods = sorted(pairMetricsByPeriod.keys())

for i in range(len(periods)-1):
    period = periods[i]

    if len(pairMetricsByPeriod[period]) < 100:
        print("Period "+str(period)+" to "+str(period+1) + " doesnt have enough samples")
        continue
    x = pairMetricsByPeriod[period][:,:-1]
    y = pairMetricsByPeriod[period][:,-1]
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42, shuffle=True)

    print("Training SVM. Year "+str(getYearFromId(period,K)))
    clf = SVC(gamma='auto')
    clf.fit(x_train, y_train)

    y_pred = clf.predict(x_test)

    pred = Counter(y_test == y_pred)

    print("Predictions: "+str(100 * pred[True]/(pred[False] + pred[True]))+"%")
    print(str(pred))
    print()

pairMetricsAndEdge = np.concatenate(list(pairMetricsByPeriod.values()))
pairMetrics =pairMetricsAndEdge[:,:-1]
isEdge = pairMetricsAndEdge[:,-1]

x = np.array(pairMetrics)
y = np.array(isEdge)
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.33, random_state=42, shuffle=True)

print()
print("Training SVM...")
clf = SVC(gamma='auto')
clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)

pred = Counter(y_test == y_pred)
print()
print("Predictions: "+str(100 * pred[True]/(pred[False] + pred[True]))+"%")
print(str(pred))